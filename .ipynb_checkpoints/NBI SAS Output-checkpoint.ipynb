{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The current process is to copy the sas data set to jupyter folder to process, \n",
    "\n",
    "* to prevent from using the original file while the original one is being written\n",
    "* need a process to transfer a copy of the file directly from project folder to juypter folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# import libraries\n",
    "# ----------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# import SAS dataset\n",
    "# ------------------\n",
    "# Please note: the import file can also be a csv file which is converted from the SAS dataset\n",
    "# PLease note: pandas may truncate infinite decimals which might cause the output accuracy might be slightly different than the output from SAS\n",
    "#df = pd.read_sas('patpop_baseline_v1.sas7bdat')\n",
    "#df = pd.read_sas('patpop_ps.sas7bdat')\n",
    "#df = pd.read_sas('patpop_baseline_add.sas7bdat')\n",
    "df = pd.read_sas('patpop_ps_add.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# create data etl rules\n",
    "# ---------------------\n",
    "\n",
    "def age_group(age):\n",
    "    # create age bin\n",
    "    if age >= 18 and age <= 54:\n",
    "        return '18-54'\n",
    "    elif age >= 55 and age <= 64:\n",
    "        return '55-64'\n",
    "    elif age >= 65 and age <= 74:\n",
    "        return '65-74'\n",
    "    else:\n",
    "        return '75+'\n",
    "\n",
    "def sex(x):\n",
    "    # create gender labels\n",
    "    if x == 1:\n",
    "        return 'male'\n",
    "    elif x == 2:\n",
    "        return 'female'\n",
    "    else:\n",
    "        return 'missing'\n",
    "\n",
    "def indicator(x):\n",
    "    # create indicator labels for numeric indicator features\n",
    "    if x == 0:\n",
    "        return 'No'\n",
    "    elif x == 1:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'Missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# apply data etl rules\n",
    "# --------------------\n",
    "\n",
    "# create feature age_group\n",
    "df['age_group'] = df['age'].apply(lambda x: age_group(x))\n",
    "# create feature sex_name\n",
    "df['sex_name'] = df['sex'].apply(lambda x: sex(x))\n",
    "# create feature c_year to get calendar year from index date\n",
    "df['c_year'] = df['index_dt'].apply(lambda x: x.to_pydatetime().year)\n",
    "# use below when using csv file to create c_year\n",
    "#df['c_year'] = df['index_dt'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# create output tables for categorical variables\n",
    "# ----------------------------------------------\n",
    "\n",
    "def cross(ind, col):\n",
    "    '''\n",
    "    purpose: return contigency tables for categorical features\n",
    "    \n",
    "    variable:\n",
    "        ind - the index for the contigency table\n",
    "        col - the column for the contigency table\n",
    "    ''' \n",
    "    return pd.crosstab(index=ind, columns=df[col])\n",
    "\n",
    "def char_std(d, ind):\n",
    "    '''\n",
    "    purpose: return std for categoical features\n",
    "    \n",
    "    variable:\n",
    "        d - the dataframe needs to be used for calculation\n",
    "        ind - the index for the output data frame which is used for merging back to the contigency table\n",
    "    '''\n",
    "    # create p1 and p2 for std calculation\n",
    "    p1 = d.iloc[0,0]/cohort_tab.iloc[0,0]\n",
    "    p2 = d.iloc[0,1]/cohort_tab.iloc[0,1]\n",
    "    \n",
    "    # calculate std\n",
    "    if p1 == 0 and p2 == 0:\n",
    "        _std_data = {'std': '.'}\n",
    "    else:\n",
    "        _std_data = {'std': np.absolute(p1 - p2)/np.sqrt((p1*(1-p1) + p2*(1-p2))/2)}\n",
    "    \n",
    "    return d.join(pd.DataFrame(_std_data, index=[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-105-99b8dbe4953c>, line 25)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-105-99b8dbe4953c>\"\u001b[1;36m, line \u001b[1;32m25\u001b[0m\n\u001b[1;33m    _nul = _nul.apply(lambda x: True if x == 0)\u001b[0m\n\u001b[1;37m                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------\n",
    "# create output tables for numeric variables\n",
    "# ------------------------------------------\n",
    "\n",
    "\n",
    "def numeric_tab(var, item, miss_trigger):\n",
    "    '''\n",
    "    purpose: return descriptive tables for numerical features\n",
    "    \n",
    "    variable:\n",
    "        var - the feature needs to be processed\n",
    "        item - the main data frame that has been grouped by cohort\n",
    "        miss_trigger - the trigger for output number of missing values\n",
    "    '''\n",
    "    # define each descriptive statistics\n",
    "    _mea = item.mean().round(2).to_frame().rename(columns={var:var+'_mean'}).transpose()\n",
    "    _s = item.std().round(2).to_frame().rename(columns={var:var+'_std'}).transpose()\n",
    "    _med = item.median().round(2).to_frame().rename(columns={var:var+'_median'}).transpose()\n",
    "    _q25 = item.quantile(0.25,interpolation='midpoint').round(2).to_frame()\n",
    "    _q75 = item.quantile(0.75,interpolation='midpoint').round(2).to_frame()\n",
    "    _min = item.min().round(2).to_frame()\n",
    "    _max = item.max().round(2).to_frame()\n",
    "    _nul = item.apply(lambda x: x.isnull().sum()).to_frame().rename(columns={var:var+'_missing'}).transpose()\n",
    "    \n",
    "    _nul = _nul.apply(lambda x: '.' if x == 0 else None)\n",
    "        \n",
    "    # combine quantiles\n",
    "    _q = _q25.rename(columns={var:var+'_Q1'}).join(_q75.rename(columns={var: var+'_Q3'}))\n",
    "    _q[var+'_Q1_Q3'] = _q[var+'_Q1'].map(str) + '; ' + _q[var+'_Q3'].map(str)\n",
    "    _q = _q.drop([var+'_Q1', var+'_Q3'], axis=1).transpose()\n",
    "    \n",
    "    # combine min and max\n",
    "    _m = _min.rename(columns={var:var+'_min'}).join(_max.rename(columns={var: var+'_max'}))\n",
    "    _m[var+'_min_max'] = _m[var+'_min'].map(str) + '; ' + _m[var+'_max'].map(str)\n",
    "    _m = _m.drop([var+'_min', var+'_max'], axis=1).transpose()\n",
    "\n",
    "    # calculate std\n",
    "    _std_data = {'std': np.absolute(_mea.iloc[0,0] - _mea.iloc[0,1])/np.sqrt((np.square(_s.iloc[0,0]) + np.square(_s.iloc[0,1]))/2)}\n",
    "    _std = pd.DataFrame(_std_data, index=[var+'_mean'])\n",
    "    \n",
    "    # control whether to output number of missing values or not\n",
    "    if miss_trigger == 1:\n",
    "        _df = pd.concat([_mea, _s, _med, _q, _m, _nul])\n",
    "    else:\n",
    "        _df = pd.concat([_mea, _s, _med, _q, _m])\n",
    "    \n",
    "    return _df.join(_std.round(3)).fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# acquire SAS variables for processing \n",
    "# ------------------------------------\n",
    "# this excel file may require proper changes before it can be processed and the rules are as follows:\n",
    "# 1.Record a MACRO for what I'm going to do\n",
    "# 2.Match each tab with corresponding output tables, create new tab if anything's not in the output table like other drugs\n",
    "# 3.Make sure all the variables names are filled in order to output NA correctly\n",
    "# 4.Make sure the output sequence matches\n",
    "# 5.Create type variable to control categorical or numerical output where C stands for categorical and N stands for Numerical\n",
    "\n",
    "sas_lifestyle = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='life_style') # not use this at this moment\n",
    "sas_Diabetes_complications = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='Diabetes complications')\n",
    "sas_other_comorbidity = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='other_comorbidity')\n",
    "sas_other_drugs = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='other_drugs')\n",
    "sas_laboratory = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='laboratory')\n",
    "sas_Prior_concomitant = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='Prior concomitant')\n",
    "sas_resource_utilization = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='resource_utilization')\n",
    "sas_cost = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guzhao\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-103-84127994d9f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcohort_tab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar_std\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number of patients'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cohort'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'number of patients'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mc_tab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar_std\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number of patients'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cohort'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'number of patients'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mage_all\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumeric_tab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'age'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cohort'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mage_tab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'age_group'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cohort'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0msex_tab\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchar_std\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'sex_name'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'cohort'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'female'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-101-344f48488ead>\u001b[0m in \u001b[0;36mnumeric_tab\u001b[1;34m(var, item, miss_trigger)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0m_nul\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mvar\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_missing'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0m_nul\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0m_nul\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__nonzero__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1476\u001b[0m         raise ValueError(\"The truth value of a {0} is ambiguous. \"\n\u001b[0;32m   1477\u001b[0m                          \u001b[1;34m\"Use a.empty, a.bool(), a.item(), a.any() or a.all().\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1478\u001b[1;33m                          .format(self.__class__.__name__))\n\u001b[0m\u001b[0;32m   1479\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1480\u001b[0m     \u001b[0m__bool__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: The truth value of a DataFrame is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all()."
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# generate output tables\n",
    "# ----------------------\n",
    "# strategy: \n",
    "#    for categorical tables, a dummy data frame will be createed first with index as 'Yes', 'No' and 'Missing', then the values will be filled in\n",
    "#    for numerical tables, the output from the function numeric_tab is immediately usable\n",
    "\n",
    "\n",
    "# prepare excel writer\n",
    "writer = pd.ExcelWriter('NBI_results_psa.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# generate \"Baseline demographics\"\n",
    "cohort_tab = char_std(cross('number of patients', 'cohort'), 'number of patients')\n",
    "c_tab = char_std(cross('number of patients', 'cohort'), 'number of patients')\n",
    "age_all = numeric_tab('age', df.groupby(['cohort']).age, 0)\n",
    "age_tab = cross(df['age_group'], 'cohort')\n",
    "sex_tab = char_std(cross(df['sex_name'], 'cohort'), 'female')\n",
    "race_dummy = pd.DataFrame({1.0:['', '', '', ''], 2.0:['', '', '', ''], 'std':['', '', '', '']}, index=['race_Category_1', 'race_Category_2', 'race_Category_3', 'race_Missing'])\n",
    "soc_dummy = pd.DataFrame({1.0:['', '', '', ''], 2.0:['', '', '', ''], 'std':['', '', '', '']}, index=['soc_Low', 'soc_Intermediate', 'soc_High', 'soc_Missing'])\n",
    "year_dummy = pd.DataFrame({1.0:[0, 0, 0], 2.0:[0, 0, 0], 'std':['.', 'NA', 'NA']}, index=['2012', '2013', '2014'])\n",
    "year_tab = cross(df['c_year'], 'cohort')\n",
    "base_tab = pd.concat([c_tab, age_all, age_tab, sex_tab, race_dummy, soc_dummy, year_dummy, year_tab])\n",
    "base_tab = base_tab.replace(np.nan, 'NA', regex=True)\n",
    "base_tab.to_excel(writer, sheet_name='baseline')\n",
    "\n",
    "# generate \"Lifestyle variables\"\n",
    "lifestyle=[]\n",
    "for value, y in zip(sas_lifestyle['Variable_name'], sas_lifestyle['Type']):\n",
    "    if value in df.columns and y == 'C':   \n",
    "        # create a dummy data frame for categorical table output\n",
    "        dummy_df = pd.DataFrame({1.0:[0, 0, '.'], 2.0:[0, 0, '.'], 'std':['.', 'NA', 'NA']}, index=['Yes', 'No', 'Missing'])\n",
    "        value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "        dummy_df.update(value_tab)\n",
    "        dummy_df.index.name = value\n",
    "        new_index = pd.DataFrame({'new_index':[value+'_Yes', value+'_No', value+'_Missing']}, index=['Yes', 'No', 'Missing'])\n",
    "        dummy_df = dummy_df.join(new_index)                        \n",
    "        dummy_df = dummy_df.reset_index(drop=True)\n",
    "        dummy_df = dummy_df.set_index('new_index')\n",
    "        lifestyle.append(dummy_df) \n",
    "    elif value in df.columns and y == 'N':\n",
    "        df_g = df.groupby(['cohort'])\n",
    "        value_tab = numeric_tab(value, df_g[value], 1)\n",
    "        lifestyle.append(value_tab)\n",
    "    elif value not in df.columns and y == 'C':\n",
    "        value_tab = pd.DataFrame({1.0:['', '', ''], 2.0:['', '', ''], 'std':['', 'NA', 'NA']}, index=[value+'_Yes', value+'_No', value+'_Missing'])\n",
    "        value_tab.index.name = value\n",
    "        lifestyle.append(value_tab)\n",
    "    elif value not in df.columns and y == 'N':\n",
    "        value_tab = pd.DataFrame({1.0:['', '', '', '', '', ''], 2.0:['', '', '', '', '', ''], 'std':['', 'NA', 'NA', 'NA', 'NA', 'NA']}, index=[value+'_mean', value+'_std', value+'_median', value+'_Q1_Q3', value+'_min_max', value+'_missing'])\n",
    "        value_tab.index.name = value    \n",
    "        lifestyle.append(value_tab)\n",
    "        \n",
    "lifestyle = pd.concat(lifestyle)\n",
    "lifestyle.to_excel(writer, sheet_name='lifestyle')\n",
    "\n",
    "\n",
    "Diabetes_complications=[]\n",
    "for value in sas_Diabetes_complications['Variable_name']:\n",
    "    if value in df.columns:\n",
    "        value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "        dummy_df = pd.DataFrame({1.0:[''], 2.0:[''], 'std':[0]}, index=['Yes'])\n",
    "        if ('Yes' in value_tab.index.values.tolist()):\n",
    "            value_tab = value_tab.loc[['Yes']]\n",
    "            dummy_df.update(value_tab)\n",
    "            dummy_df.index.name = value\n",
    "            new_index = pd.DataFrame({'new_index':[value+'_Yes']}, index=['Yes'])\n",
    "            dummy_df = dummy_df.join(new_index)                        \n",
    "            dummy_df = dummy_df.reset_index(drop=True)\n",
    "            dummy_df = dummy_df.set_index('new_index')\n",
    "            Diabetes_complications.append(dummy_df)\n",
    "        else:\n",
    "            value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'Yes'])\n",
    "            value_tab.index.name = value\n",
    "            Diabetes_complications.append(value_tab)\n",
    "    else:\n",
    "        value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'Yes'])\n",
    "        value_tab.index.name = value    \n",
    "        Diabetes_complications.append(value_tab)    \n",
    "\n",
    "Diabetes_complications = pd.concat(Diabetes_complications)\n",
    "Diabetes_complications.to_excel(writer, sheet_name='Diabetes_complications')\n",
    "\n",
    "other_comorbidity=[]\n",
    "for value in sas_other_comorbidity['Variable_name']:\n",
    "    if value in df.columns:\n",
    "        value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "        dummy_df = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=['Yes'])\n",
    "        if ('Yes' in value_tab.index.values.tolist()):\n",
    "            value_tab = value_tab.loc[['Yes']]\n",
    "            dummy_df.update(value_tab)\n",
    "            dummy_df.index.name = value\n",
    "            new_index = pd.DataFrame({'new_index':[value+'_Yes']}, index=['Yes'])\n",
    "            dummy_df = dummy_df.join(new_index)                        \n",
    "            dummy_df = dummy_df.reset_index(drop=True)\n",
    "            dummy_df = dummy_df.set_index('new_index')\n",
    "            other_comorbidity.append(dummy_df) \n",
    "        else:\n",
    "            value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'Yes'])\n",
    "            value_tab.index.name = value\n",
    "            other_comorbidity.append(value_tab) \n",
    "    else:\n",
    "        value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'Yes'])\n",
    "        value_tab.index.name = value\n",
    "        other_comorbidity.append(value_tab)   \n",
    "\n",
    "other_comorbidity = pd.concat(other_comorbidity)\n",
    "other_comorbidity.to_excel(writer, sheet_name='other_comorbidity')\n",
    "\n",
    "\n",
    "laboratory=[]\n",
    "for value in sas_laboratory['Variable_name_res']:\n",
    "    if value in df.columns:\n",
    "        df_g = df.groupby(['cohort'])\n",
    "        value_tab = numeric_tab(value, df_g[value], 1)\n",
    "        laboratory.append(value_tab)  \n",
    "    else:\n",
    "        value_tab = pd.DataFrame({1.0:['', '', '', '', '', ''], \n",
    "                                  2.0:['', '', '', '', '', ''], \n",
    "                                  'std':['', 'NA', 'NA', 'NA', 'NA', 'NA']}, \n",
    "                                  index=[value+'_mean', value+'_std', value+'_median', value+'_Q1_Q3', value+'_min_max', value+'_missing'])\n",
    "        value_tab.index.name = value    \n",
    "        laboratory.append(value_tab)    \n",
    "\n",
    "laboratory = pd.concat(laboratory)\n",
    "laboratory.to_excel(writer, sheet_name='laboratory')\n",
    "\n",
    "\n",
    "other_drugs=[]\n",
    "for value in sas_other_drugs['Variable_name']:\n",
    "    if value in df.columns:\n",
    "        value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "        dummy_df = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=['Yes'])\n",
    "        if ('Yes' in value_tab.index.values.tolist()):\n",
    "            value_tab = value_tab.loc[['Yes']]\n",
    "            dummy_df.update(value_tab)\n",
    "            dummy_df.index.name = value\n",
    "            new_index = pd.DataFrame({'new_index':[value+'_Yes']}, index=['Yes'])\n",
    "            dummy_df = dummy_df.join(new_index)                        \n",
    "            dummy_df = dummy_df.reset_index(drop=True)\n",
    "            dummy_df = dummy_df.set_index('new_index')\n",
    "            other_drugs.append(dummy_df)  \n",
    "        else:\n",
    "            value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'_Yes'])\n",
    "            value_tab.index.name = value\n",
    "            other_drugs.append(value_tab)  \n",
    "\n",
    "    else:\n",
    "        value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'_Yes'])\n",
    "        value_tab.index.name = value\n",
    "        other_drugs.append(value_tab)  \n",
    "\n",
    "other_drugs = pd.concat(other_drugs)\n",
    "other_drugs.to_excel(writer, sheet_name='other_drugs')\n",
    "\n",
    "\n",
    "Prior_concomitant=[]\n",
    "for value, y in zip(sas_Prior_concomitant['variable_name'], sas_Prior_concomitant['Type']):\n",
    "    if value in df.columns and y == 'C':\n",
    "        value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "        dummy_df = pd.DataFrame({1.0:['.'], 2.0:['.'], 'std':['.']}, index=['Yes'])\n",
    "        if ('Yes' in value_tab.index.values.tolist()):\n",
    "            value_tab = value_tab.loc[['Yes']]\n",
    "            dummy_df.update(value_tab)\n",
    "            dummy_df.index.name = value\n",
    "            new_index = pd.DataFrame({'new_index':[value+'_Yes']}, index=['Yes'])\n",
    "            dummy_df = dummy_df.join(new_index)                        \n",
    "            dummy_df = dummy_df.reset_index(drop=True)\n",
    "            dummy_df = dummy_df.set_index('new_index')\n",
    "            Prior_concomitant.append(dummy_df)\n",
    "        else:\n",
    "            value_tab = pd.DataFrame({1.0:['.'], 2.0:['.'], 'std':['.']}, index=[value+'_Yes'])\n",
    "            value_tab.index.name = value\n",
    "            Prior_concomitant.append(value_tab)\n",
    "    elif value in df.columns and y == 'N':\n",
    "        df_g = df.groupby(['cohort'])\n",
    "        value_tab = numeric_tab(value, df_g[value], 0)\n",
    "        Prior_concomitant.append(value_tab)\n",
    "    elif value not in df.columns and y == 'C':\n",
    "        value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'_Yes'])\n",
    "        value_tab.index.name = value\n",
    "        Prior_concomitant.append(value_tab)\n",
    "    elif value not in df.columns and y == 'N':\n",
    "        value_tab = pd.DataFrame({1.0:['', '', '', '', '', ''], \n",
    "                                  2.0:['', '', '', '', '', ''], \n",
    "                                  'std':['', 'NA', 'NA', 'NA', 'NA', 'NA']}, \n",
    "                                  index=['_mean', '_std', '_median', '_Q1_Q3', '_min_max', '_missing'])\n",
    "        value_tab.index.name = value    \n",
    "        Prior_concomitant.append(value_tab)  \n",
    "\n",
    "Prior_concomitant = pd.concat(Prior_concomitant)\n",
    "Prior_concomitant.to_excel(writer, sheet_name='Prior_concomitant')\n",
    "\n",
    "\n",
    "resource_utilization=[]\n",
    "for value, y in zip(sas_resource_utilization['variable_name'], sas_resource_utilization['Type']):\n",
    "    if value in df.columns and y == 'C':\n",
    "        value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "        dummy_df = pd.DataFrame({1.0:[0, 0, '.'], 2.0:[0, 0, '.'], 'std':['.', 'NA', 'NA']}, index=['Yes', 'No', 'Missing'])\n",
    "        if ('Yes' or 'No' in value_tab.index.values.tolist()):\n",
    "            dummy_df.update(value_tab)\n",
    "            dummy_df.index.name = value\n",
    "            new_index = pd.DataFrame({'new_index':[value+'_Yes', value+'_No', value+'_Missing']}, index=['Yes', 'No', 'Missing'])\n",
    "            dummy_df = dummy_df.join(new_index)                        \n",
    "            dummy_df = dummy_df.reset_index(drop=True)\n",
    "            dummy_df = dummy_df.set_index('new_index')\n",
    "            resource_utilization.append(dummy_df)\n",
    "        else:\n",
    "            value_tab = pd.DataFrame({1.0:[0, 0, '.'], 2.0:[0, 0, '.'], 'std':['.', 0, 0]}, index=[value+'_Yes', value+'_No', value+'_Missing'])\n",
    "            value_tab.index.name = value\n",
    "            resource_utilization.append(value_tab)    \n",
    "    elif value in df.columns and y == 'N':\n",
    "        df_g = df.groupby(['cohort'])\n",
    "        value_tab = numeric_tab(value, df_g[value], 1)\n",
    "        resource_utilization.append(value_tab)\n",
    "    elif value not in df.columns and y == 'C':\n",
    "        value_tab = pd.DataFrame({1.0:['', '', ''], 2.0:['', '', ''], 'std':['', 'NA', 'NA']}, index=[value+'_Yes', value+'_No', value+'_Missing'])\n",
    "        value_tab.index.name = value\n",
    "        resource_utilization.append(value_tab)\n",
    "    elif value not in df.columns and y == 'N':\n",
    "        value_tab = pd.DataFrame({1.0:['', '', '', '', '', ''], \n",
    "                                  2.0:['', '', '', '', '', ''], \n",
    "                                  'std':['', 'NA', 'NA', 'NA', 'NA', 'NA']}, \n",
    "                                  index=[value+'_mean', value+'_std', value+'_median', value+'_Q1_Q3', value+'_min_max', value+'_missing'])\n",
    "        value_tab.index.name = value    \n",
    "        resource_utilization.append(value_tab)\n",
    "        \n",
    "resource_utilization = pd.concat(resource_utilization)\n",
    "resource_utilization.to_excel(writer, sheet_name='resource_utilization')\n",
    "\n",
    "\n",
    "cost=[]\n",
    "for value in sas_cost['variable_name']:\n",
    "    if value in df.columns:\n",
    "        df_g = df.groupby(['cohort'])\n",
    "        value_tab = numeric_tab(value, df_g[value], 1)\n",
    "        cost.append(value_tab)  \n",
    "    else:\n",
    "        value_tab = pd.DataFrame({1.0:['', '', '', '', '', ''], \n",
    "                                  2.0:['', '', '', '', '', ''], \n",
    "                                  'std':['', 'NA', 'NA', 'NA', 'NA', 'NA']}, \n",
    "                                  index=[value+'_mean', value+'_std', value+'_median', value+'_Q1_Q3', value+'_min_max', value+'_missing'])\n",
    "        value_tab.index.name = value    \n",
    "        cost.append(value_tab)    \n",
    "\n",
    "cost = pd.concat(cost)\n",
    "cost.to_excel(writer, sheet_name='cost')\n",
    "\n",
    "\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guzhao\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1.0</th>\n",
       "      <th>2.0</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number of patients</th>\n",
       "      <td>5110</td>\n",
       "      <td>5110</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1.0   2.0  std\n",
       "row_0                              \n",
       "number of patients  5110  5110  NaN"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cohort_tab = char_std(cross('number of patients', 'cohort'), 'number of patients')\n",
    "cohort_tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddd = numeric_tab(var, item, miss_trigger)\n",
    "ddd"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
