{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\">NBI SAS Output</h1>\n",
    "\n",
    "###### The current process is to generate output for NBI Table shell\n",
    "\n",
    "**Instruction**\n",
    "* Copy the original SAS dataset first\n",
    "* Import SAS dataset\n",
    "* Process the code and it generates a excel file which resembles to the table shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# import libraries\n",
    "# ----------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import datetime\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "┌──────────────────┬───────────────┬──────────────────┬──────────────┬───────────┐\n",
    "│     Function     │Copies metadata│Copies permissions│Can use buffer│Dest dir OK│\n",
    "├──────────────────┼───────────────┼──────────────────┼──────────────┼───────────┤\n",
    "│shutil.copy       │      No       │        Yes       │    No        │    Yes    │\n",
    "│shutil.copyfile   │      No       │        No        │    No        │    No     │\n",
    "│shutil.copy2      │      Yes      │        Yes       │    No        │    Yes    │\n",
    "│shutil.copyfileobj│      No       │        No        │    Yes       │    No     │\n",
    "└──────────────────┴───────────────┴──────────────────┴──────────────┴───────────┘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------\n",
    "# import SAS dataset\n",
    "# ------------------\n",
    "# Please note: the import file can also be a csv file which is converted from the SAS dataset\n",
    "# PLease note: pandas may truncate infinite decimals which might cause the output accuracy might be slightly different than the output from SAS\n",
    "\n",
    "file_path = '//prod-isilon.corporate.ivh/SASDataHome/SASProduction/Boehringer Ingelheim/EMPRISE/SASData/Empa_DPP4_main/'\n",
    "# doing the following is not suggest as it's processing on the orignal file.\n",
    "#test_df = pd.read_sas(file_path+'patpop_ps_add.sas7bdat')\n",
    "dest_path = 'C:/Users/guzhao/Documents/GitHub/Code_Generator_SAS/Code-Generator-SAS/'\n",
    "# uncomment this to copy the file to the juypter folder\n",
    "#copyfile(file_path+\"patpop_ps_add.sas7bdat\", dest_path+'patpop_ps_add.sas7bdat')\n",
    "\n",
    "#df = pd.read_sas('patpop_baseline_v1.sas7bdat')\n",
    "#df = pd.read_sas('patpop_ps.sas7bdat')\n",
    "#df = pd.read_sas('patpop_baseline_add.sas7bdat')\n",
    "df = pd.read_sas('patpop_ps_add.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------\n",
    "# create data etl rules\n",
    "# ---------------------\n",
    "\n",
    "def age_group(age):\n",
    "    # create age bin\n",
    "    if age >= 18 and age <= 54:\n",
    "        return '18-54'\n",
    "    elif age >= 55 and age <= 64:\n",
    "        return '55-64'\n",
    "    elif age >= 65 and age <= 74:\n",
    "        return '65-74'\n",
    "    else:\n",
    "        return '75+'\n",
    "\n",
    "def sex(x):\n",
    "    # create gender labels\n",
    "    if x == 1:\n",
    "        return 'male'\n",
    "    elif x == 2:\n",
    "        return 'female'\n",
    "    else:\n",
    "        return 'missing'\n",
    "\n",
    "def indicator(x):\n",
    "    # create indicator labels for numeric indicator features\n",
    "    if x == 0:\n",
    "        return 'No'\n",
    "    elif x == 1:\n",
    "        return 'Yes'\n",
    "    else:\n",
    "        return 'Missing'\n",
    "    \n",
    "def dot(x):\n",
    "    # convert missing values from 0 to .\n",
    "    if x == 0:\n",
    "        return '.'\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------\n",
    "# apply data etl rules\n",
    "# --------------------\n",
    "\n",
    "# create feature age_group\n",
    "df['age_group'] = df['age'].apply(lambda x: age_group(x))\n",
    "# create feature sex_name\n",
    "df['sex_name'] = df['sex'].apply(lambda x: sex(x))\n",
    "# create feature c_year to get calendar year from index date\n",
    "df['c_year'] = df['index_dt'].apply(lambda x: x.to_pydatetime().year)\n",
    "\n",
    "# use below when using csv file to create c_year\n",
    "#df['c_year'] = df['index_dt'].astype(str).str[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------\n",
    "# create output tables for categorical variables\n",
    "# ----------------------------------------------\n",
    "\n",
    "def cross(ind, col):\n",
    "    '''\n",
    "    purpose: return contigency tables for categorical features\n",
    "    \n",
    "    variable:\n",
    "        ind - the index for the contigency table\n",
    "        col - the column for the contigency table\n",
    "    ''' \n",
    "    return pd.crosstab(index=ind, columns=df[col])\n",
    "\n",
    "def char_std(d, ind):\n",
    "    '''\n",
    "    purpose: return std for categoical features\n",
    "    \n",
    "    variable:\n",
    "        d - the dataframe needs to be used for calculation\n",
    "        ind - the index for the output data frame which is used for merging back to the contigency table\n",
    "    '''\n",
    "    # create p1 and p2 for std calculation\n",
    "    p1 = d.iloc[0,0]/cohort_tab.iloc[0,0]\n",
    "    p2 = d.iloc[0,1]/cohort_tab.iloc[0,1]\n",
    "    \n",
    "    # calculate std\n",
    "    if p1 == 0 and p2 == 0:\n",
    "        _std_data = {'std': '.'}\n",
    "    else:\n",
    "        _std_data = {'std': np.absolute(p1 - p2)/np.sqrt((p1*(1-p1) + p2*(1-p2))/2)}\n",
    "    \n",
    "    return d.join(pd.DataFrame(_std_data, index=[ind])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------\n",
    "# create output tables for numeric variables\n",
    "# ------------------------------------------\n",
    "\n",
    "\n",
    "def numeric_tab(var, item, miss_trigger):\n",
    "    '''\n",
    "    purpose: return descriptive tables for numerical features\n",
    "    \n",
    "    variable:\n",
    "        var - the feature needs to be processed\n",
    "        item - the main data frame that has been grouped by cohort\n",
    "        miss_trigger - the trigger for output number of missing values\n",
    "    '''\n",
    "    # define each descriptive statistics\n",
    "    _mea = item.mean().round(2).to_frame().rename(columns={var:var+'_mean'}).transpose()\n",
    "    _s = item.std().round(2).to_frame().rename(columns={var:var+'_std'}).transpose()\n",
    "    _med = item.median().round(2).to_frame().rename(columns={var:var+'_median'}).transpose()\n",
    "    _q25 = item.quantile(0.25,interpolation='midpoint').round(2).to_frame()\n",
    "    _q75 = item.quantile(0.75,interpolation='midpoint').round(2).to_frame()\n",
    "    _min = item.min().round(2).to_frame()\n",
    "    _max = item.max().round(2).to_frame()\n",
    "    _nul = item.apply(lambda x: x.isnull().sum()).apply(lambda x: dot(x)).to_frame().rename(columns={var:var+'_missing'}).transpose()\n",
    "        \n",
    "    # combine quantiles\n",
    "    _q = _q25.rename(columns={var:var+'_Q1'}).join(_q75.rename(columns={var: var+'_Q3'}))\n",
    "    _q[var+'_Q1_Q3'] = _q[var+'_Q1'].map(str) + '; ' + _q[var+'_Q3'].map(str)\n",
    "    _q = _q.drop([var+'_Q1', var+'_Q3'], axis=1).transpose()\n",
    "    \n",
    "    # combine min and max\n",
    "    _m = _min.rename(columns={var:var+'_min'}).join(_max.rename(columns={var: var+'_max'}))\n",
    "    _m[var+'_min_max'] = _m[var+'_min'].map(str) + '; ' + _m[var+'_max'].map(str)\n",
    "    _m = _m.drop([var+'_min', var+'_max'], axis=1).transpose()\n",
    "\n",
    "    # calculate std\n",
    "    _std_data = {'std': np.absolute(_mea.iloc[0,0] - _mea.iloc[0,1])/np.sqrt((np.square(_s.iloc[0,0]) + np.square(_s.iloc[0,1]))/2)}\n",
    "    _std = pd.DataFrame(_std_data, index=[var+'_mean'])\n",
    "    \n",
    "    # control whether to output number of missing values or not\n",
    "    if miss_trigger == 1:\n",
    "        _df = pd.concat([_mea, _s, _med, _q, _m, _nul])\n",
    "    else:\n",
    "        _df = pd.concat([_mea, _s, _med, _q, _m])\n",
    "    \n",
    "    return _df.join(_std.round(3)).fillna('NA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------\n",
    "# acquire SAS variables for processing \n",
    "# ------------------------------------\n",
    "# this excel file may require proper changes before it can be processed and the rules are as follows:\n",
    "# 1.Record a MACRO for what I'm going to do\n",
    "# 2.Match each tab with corresponding output tables, create new tab if anything's not in the output table like other drugs\n",
    "# 3.Make sure all the variables names are filled in order to output NA correctly\n",
    "# 4.Make sure the output sequence matches\n",
    "# 5.Create type variable to control categorical or numerical output where C stands for categorical and N stands for Numerical\n",
    "\n",
    "sas_lifestyle = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='life_style') # not use this at this moment\n",
    "sas_Diabetes_complications = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='Diabetes complications')\n",
    "sas_other_comorbidity = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='other_comorbidity')\n",
    "sas_other_drugs = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='other_drugs')\n",
    "sas_laboratory = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='laboratory')\n",
    "sas_Prior_concomitant = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='Prior concomitant')\n",
    "sas_resource_utilization = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='resource_utilization')\n",
    "sas_cost = pd.read_excel('NBI covariates name 1 .xlsx', sheet_name='cost')\n",
    "\n",
    "\n",
    "sas_dummy = pd.DataFrame({'Covariates in table shells ':[''], 'Variable_name':[''], 'Type':[''], 'Style':['']}, index=['placeholder'])\n",
    "\n",
    "sas_var = pd.concat([sas_dummy, sas_lifestyle, sas_dummy, sas_Diabetes_complications, sas_dummy, \n",
    "                     sas_other_comorbidity, sas_dummy, sas_laboratory, sas_dummy,\n",
    "                     sas_Prior_concomitant, sas_dummy, sas_other_drugs, sas_dummy,\n",
    "                     sas_resource_utilization, sas_dummy, sas_cost])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\guzhao\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# generate output tables\n",
    "# ----------------------\n",
    "# strategy: \n",
    "#    for categorical tables, a dummy data frame will be createed first with index as 'Yes', 'No' and 'Missing', then the output values will be filled in\n",
    "#    for numerical tables, the output from the function numeric_tab is immediately usable\n",
    "\n",
    "\n",
    "# prepare excel writer\n",
    "writer = pd.ExcelWriter('NBI_results_psa.xlsx', engine='xlsxwriter')\n",
    "\n",
    "# generate \"Baseline demographics\"\n",
    "cohort_tab = cross('number of patients', 'cohort')\n",
    "c_tab = char_std(cohort_tab, 'number of patients')\n",
    "age_all = numeric_tab('age', df.groupby(['cohort']).age, 0)\n",
    "age_tab = cross(df['age_group'], 'cohort')\n",
    "sex_tab = char_std(cross(df['sex_name'], 'cohort'), 'female')\n",
    "race_dummy = pd.DataFrame({1.0:['', '', '', ''], 2.0:['', '', '', ''], 'std':['', '', '', '']}, index=['race_Category_1', 'race_Category_2', 'race_Category_3', 'race_Missing'])\n",
    "soc_dummy = pd.DataFrame({1.0:['', '', '', ''], 2.0:['', '', '', ''], 'std':['', '', '', '']}, index=['soc_Low', 'soc_Intermediate', 'soc_High', 'soc_Missing'])\n",
    "year_dummy = pd.DataFrame({1.0:[0, 0, 0, 0, 0, 0, 0], 2.0:[0, 0, 0, 0, 0, 0, 0], 'std':['.', 'NA', 'NA', 'NA', 'NA', 'NA', 'NA']}, index=[2012, 2013, 2014, 2015, 2016, 2017, 2018])\n",
    "year_tab = cross(df['c_year'], 'cohort')\n",
    "year_dummy.update(year_tab)\n",
    "\n",
    "# generate \"the rest of the output\"\n",
    "output=[]\n",
    "for value, t, s in zip(sas_var['Variable_name'], sas_var['Type'], sas_var['Style']):\n",
    "    if value in df.columns and t == 'C':   \n",
    "        \n",
    "        if s == 'F':\n",
    "            dummy_df = pd.DataFrame({1.0:[0, 0, '.'], 2.0:[0, 0, '.'], 'std':['.', 'NA', 'NA']}, index=['Yes', 'No', 'Missing'])\n",
    "            value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "            new_index = pd.DataFrame({'new_index':[value+'_Yes', value+'_No', value+'_Missing']}, index=['Yes', 'No', 'Missing'])\n",
    "        elif s == 'Y':\n",
    "            dummy_df = pd.DataFrame({1.0:[0], 2.0:[0], 'std':['.']}, index=['Yes'])\n",
    "            value_tab = char_std(cross(df[value].apply(lambda x:indicator(x)), 'cohort').iloc[::-1], 'Yes')\n",
    "            if ('Yes' in value_tab.index.values.tolist()):\n",
    "                value_tab = value_tab.loc[['Yes']]\n",
    "            else:\n",
    "                value_tab = pd.DataFrame({1.0:[0], 2.0:[0], 'std':['.']}, index=[value+'_Yes'])\n",
    "            new_index = pd.DataFrame({'new_index':[value+'_Yes']}, index=['Yes'])\n",
    "        \n",
    "        dummy_df.update(value_tab)\n",
    "        dummy_df.index.name = value   \n",
    "        dummy_df = dummy_df.join(new_index)                        \n",
    "        dummy_df = dummy_df.reset_index(drop=True)\n",
    "        dummy_df = dummy_df.set_index('new_index')    \n",
    "        output.append(dummy_df) \n",
    "    elif value in df.columns and t == 'N' and s == 'Y':\n",
    "        df_g = df.groupby(['cohort'])\n",
    "        value_tab = numeric_tab(value, df_g[value], 1)\n",
    "        output.append(value_tab)\n",
    "    elif value in df.columns and t == 'N' and s == 'N':\n",
    "        df_g = df.groupby(['cohort'])\n",
    "        value_tab = numeric_tab(value, df_g[value], 0)\n",
    "        output.append(value_tab)\n",
    "    elif value not in df.columns and t == 'C':\n",
    "        if s == 'F':\n",
    "            value_tab = pd.DataFrame({1.0:['', '', ''], 2.0:['', '', ''], 'std':['', 'NA', 'NA']}, index=[value+'_Yes', value+'_No', value+'_Missing'])\n",
    "        elif s == 'Y':\n",
    "            value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'_Yes'])\n",
    "\n",
    "        value_tab.index.name = value      \n",
    "        output.append(value_tab)\n",
    "    elif value not in df.columns and t == 'N':\n",
    "        value_tab = pd.DataFrame({1.0:['', '', '', '', '', ''], 2.0:['', '', '', '', '', ''], 'std':['', 'NA', 'NA', 'NA', 'NA', 'NA']}, index=[value+'_mean', value+'_std', value+'_median', value+'_Q1_Q3', value+'_min_max', value+'_missing'])\n",
    "        value_tab.index.name = value    \n",
    "        output.append(value_tab)\n",
    "    else:\n",
    "        value_tab = pd.DataFrame({1.0:[''], 2.0:[''], 'std':['']}, index=[value+'_placeholder'])\n",
    "        value_tab.index.name = value    \n",
    "        output.append(value_tab)\n",
    "\n",
    "output = pd.concat(output)        \n",
    "_output = pd.concat([c_tab, age_all, age_tab, sex_tab, race_dummy, soc_dummy, year_dummy, output])\n",
    "_output = _output.replace(np.nan, 'NA', regex=True)\n",
    "_output.to_excel(writer, sheet_name='output')\n",
    "\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
